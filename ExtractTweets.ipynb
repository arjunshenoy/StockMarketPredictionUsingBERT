{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtractTweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import os\n",
        "import time\n"
      ],
      "metadata": {
        "id": "JwIKAGVFrnPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Twitter developer credentials"
      ],
      "metadata": {
        "id": "0vhvFWYU6fq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter credentials\n",
        "# Obtain them from your twitter developer account\n",
        "consumer_key = 'DNvrqniGAvAaSVqfwI1nd9Iy1'\n",
        "consumer_secret = '8xma1rxgBWBzsOwxl02XuB1vBrjgPXGJgCeCLXfkQIuEeJpqbE'\n",
        "access_key = '1499145505481404421-eUvIVTJf8CfmzMu248iK5wPRxajD9a'\n",
        "access_secret = 'ESMAC8i3a9JWsTrMDqX9DDnjR8mwBoj9qWvlee3JeNDYp'\n"
      ],
      "metadata": {
        "id": "RKzqwVaY6kRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Authenticating with twitter using the credentials supplied above"
      ],
      "metadata": {
        "id": "ALTsRi2i6mph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Twitter authentication\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)   \n",
        "auth.set_access_token(access_key, access_secret) \n",
        "  \n",
        "# Creating an API object \n",
        "api = tweepy.API(auth)\n",
        "def  authenticate_user():\n",
        "\n",
        "  try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication with twitter successfull\")\n",
        "  except:\n",
        "    print(\"Authentication failed\")"
      ],
      "metadata": {
        "id": "dtCcrfbWnH9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to write tweets to CSV files"
      ],
      "metadata": {
        "id": "wmlVTMWl6u-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToCSV(db_tweets, type_of_tweet):  \n",
        "    from datetime import datetime\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    path = os.getcwd()\n",
        "    filename = 'Tweets'+'_'+type_of_tweet+'.csv'\n",
        "\n",
        "    db_tweets.to_csv(filename, index = False)"
      ],
      "metadata": {
        "id": "GruIkVF2-t20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to scrap tweets using hashtags - Using Search API"
      ],
      "metadata": {
        "id": "kRLuQZAK6z6a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgyGkOi5mAM1"
      },
      "outputs": [],
      "source": [
        "def scrapTweetsByTags(search_words, date_since, numTweets, numRuns):\n",
        "\n",
        "    db_tweets = pd.DataFrame(columns = ['username',  'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags'])\n",
        "    \n",
        "    for i in range(0, numRuns):\n",
        "        start_run = time.time()\n",
        "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", until=\"2022-06-02\", tweet_mode='extended', wait_on_rate_limit=True).items(numTweets)\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "            username = tweet.user.screen_name\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            ith_tweet = [username, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "            noTweets += 1\n",
        "        \n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
        "        \n",
        "        #time.sleep(2) #15 minute sleep time\n",
        "    return db_tweets\n",
        "    print('Scraping has completed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to scrap tweets of a particular user using the user_timeline API"
      ],
      "metadata": {
        "id": "J3IAoyuS69Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapTweetsByUser(user_name, search_words, date_since, numTweets, numRuns):\n",
        "\n",
        "    db_tweets_user = pd.DataFrame(columns = ['username', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags'])\n",
        "    \n",
        "    for user in user_name:\n",
        "        start_run = time.time()\n",
        "        tweets = tweepy.Cursor(api.user_timeline,screen_name = user ,lang=\"en\", since=\"2020-01-01\",until=\"2020-01-02\",  tweet_mode='extended', wait_on_rate_limit=True).items(numTweets)\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "            username = tweet.user.screen_name\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            ith_tweet = [username, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            db_tweets_user.loc[len(db_tweets_user)] = ith_tweet\n",
        "            noTweets += 1\n",
        "        \n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for USER {} is {}'.format(user, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(user, duration_run))\n",
        "        \n",
        "        #time.sleep(5) #15 minute sleep time\n",
        "    return db_tweets_user\n",
        "    print('Scraping has completed!')"
      ],
      "metadata": {
        "id": "os4lTIJls-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Invoking the necessary functions to scrap tweets"
      ],
      "metadata": {
        "id": "wwTPdyzy7Gsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_words = \"#AMZN OR $AMZN OR  #amzn OR #techstocks OR #stockmarket OR #NYSE OR #investing OR #Stocks\"\n",
        "#search_words = \"#NFLX\"\n",
        "#user_name = [\"CNBC\",\"Benzinga\",\"Stocktwits\",\"SJosephBurns\",\"BreakoutStocks\",\"bespokeinvest\",\"WSJMarkets\",\"Stephanie_Link\",\"nytimesbusiness\",\"IBDinvestors\",\"WSJDealJournal\",\"MarketWatch\"]\n",
        "user_name = [\"CNBC\"]\n",
        "date_since = \"2000-01-01\"\n",
        "filter_retweets =\" -filter:retweets\"\n",
        "numTweets = 20000\n",
        "numRuns =1\n",
        "\n",
        "\n",
        "authenticate_user()\n",
        "#tweets =  scrapTweetsByTags(search_words, date_since, numTweets, numRuns)\n",
        "tweets =  scrapTweetsByUser(user_name,search_words, date_since, numTweets, numRuns)\n",
        "writeToCSV(tweets,'Tags')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FMB0OLTmbCB",
        "outputId": "787c2f5f-d392-459d-feed-3fd668694bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication with twitter successfull\n",
            "no. of tweets scraped for USER CNBC is 280\n",
            "time take for CNBC run to complete is 5.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We were not able to obtain tweets which were older than 6 weeks using the APIs used above and after doing propert research it was found that it was one of the limitations of the twitter developer APIs and hence other open source datasets were used to obtain the tweets"
      ],
      "metadata": {
        "id": "QjTUS-tO7Lc3"
      }
    }
  ]
}