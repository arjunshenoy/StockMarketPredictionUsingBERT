{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtractTweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import os\n",
        "import time\n",
        "# Twitter credentials\n",
        "# Obtain them from your twitter developer account\n",
        "consumer_key = 'DNvrqniGAvAaSVqfwI1nd9Iy1'\n",
        "consumer_secret = '8xma1rxgBWBzsOwxl02XuB1vBrjgPXGJgCeCLXfkQIuEeJpqbE'\n",
        "access_key = '1499145505481404421-eUvIVTJf8CfmzMu248iK5wPRxajD9a'\n",
        "access_secret = 'ESMAC8i3a9JWsTrMDqX9DDnjR8mwBoj9qWvlee3JeNDYp'\n"
      ],
      "metadata": {
        "id": "JwIKAGVFrnPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Twitter authentication\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)   \n",
        "auth.set_access_token(access_key, access_secret) \n",
        "  \n",
        "# Creating an API object \n",
        "api = tweepy.API(auth)\n",
        "def  authenticate_user():\n",
        "\n",
        "  try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication with twitter successfull\")\n",
        "  except:\n",
        "    print(\"Authentication failed\")"
      ],
      "metadata": {
        "id": "dtCcrfbWnH9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeToCSV(db_tweets, type_of_tweet):  \n",
        "    from datetime import datetime\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    path = os.getcwd()\n",
        "    filename = 'Tweets'+'_'+type_of_tweet+'.csv'\n",
        "\n",
        "    db_tweets.to_csv(filename, index = False)"
      ],
      "metadata": {
        "id": "GruIkVF2-t20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgyGkOi5mAM1"
      },
      "outputs": [],
      "source": [
        "def scrapTweetsByTags(search_words, date_since, numTweets, numRuns):\n",
        "\n",
        "    db_tweets = pd.DataFrame(columns = ['username',  'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags'])\n",
        "    \n",
        "    for i in range(0, numRuns):\n",
        "        start_run = time.time()\n",
        "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", until=\"2022-06-02\", tweet_mode='extended', wait_on_rate_limit=True).items(numTweets)\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "            username = tweet.user.screen_name\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            ith_tweet = [username, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "            noTweets += 1\n",
        "        \n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
        "        \n",
        "        #time.sleep(2) #15 minute sleep time\n",
        "    return db_tweets\n",
        "    print('Scraping has completed!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrapTweetsByUser(user_name, search_words, date_since, numTweets, numRuns):\n",
        "\n",
        "    db_tweets_user = pd.DataFrame(columns = ['username', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags'])\n",
        "    \n",
        "    for user in user_name:\n",
        "        start_run = time.time()\n",
        "        tweets = tweepy.Cursor(api.user_timeline,screen_name = user ,lang=\"en\", since=\"2020-01-01\",until=\"2020-01-02\",  tweet_mode='extended', wait_on_rate_limit=True).items(numTweets)\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "            username = tweet.user.screen_name\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            ith_tweet = [username, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            db_tweets_user.loc[len(db_tweets_user)] = ith_tweet\n",
        "            noTweets += 1\n",
        "        \n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for USER {} is {}'.format(user, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(user, duration_run))\n",
        "        \n",
        "        #time.sleep(5) #15 minute sleep time\n",
        "    return db_tweets_user\n",
        "    print('Scraping has completed!')"
      ],
      "metadata": {
        "id": "os4lTIJls-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_words = \"#AMZN OR $AMZN OR  #amzn OR #techstocks OR #stockmarket OR #NYSE OR #investing OR #Stocks\"\n",
        "#search_words = \"#NFLX\"\n",
        "#user_name = [\"CNBC\",\"Benzinga\",\"Stocktwits\",\"SJosephBurns\",\"BreakoutStocks\",\"bespokeinvest\",\"WSJMarkets\",\"Stephanie_Link\",\"nytimesbusiness\",\"IBDinvestors\",\"WSJDealJournal\",\"MarketWatch\"]\n",
        "user_name = [\"CNBC\"]\n",
        "date_since = \"2000-01-01\"\n",
        "filter_retweets =\" -filter:retweets\"\n",
        "numTweets = 20000\n",
        "numRuns =1\n",
        "\n",
        "\n",
        "authenticate_user()\n",
        "#tweets =  scrapTweetsByTags(search_words, date_since, numTweets, numRuns)\n",
        "tweets =  scrapTweetsByUser(user_name,search_words, date_since, numTweets, numRuns)\n",
        "writeToCSV(tweets,'Tags')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FMB0OLTmbCB",
        "outputId": "787c2f5f-d392-459d-feed-3fd668694bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication with twitter successfull\n",
            "no. of tweets scraped for USER CNBC is 280\n",
            "time take for CNBC run to complete is 5.59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qmHDOoNQ11A8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}